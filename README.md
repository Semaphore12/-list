[1] Huang X, Franke G, Yang Z, et al. Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers[J]. arXiv preprint arXiv:2509.03059, 2025.  [[pdf]](https://arxiv.org/abs/2509.03059v1) [[code]](https://github.com/camel-ai/loong)

[2] Han Y, Zhang C, Chen X, et al. Chartllama: A multimodal llm for chart understanding and generation[J]. arXiv preprint arXiv:2311.16483, 2023. [[pdf]](https://arxiv.org/abs/2311.16483) [[code]](https://tingxueronghua.github.io/ChartLlama/)

[3] Wang Z, Xia M, He L, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms[J]. Advances in Neural Information Processing Systems, 2024, 37: 113569-113697. [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2024/hash/cdf6f8e9fd9aeaf79b6024caec24f15b-Abstract-Datasets_and_Benchmarks_Track.html) [[code]](https://charxiv.github.io/)

[4] Patel A, Raffel C, Callison-Burch C. Datadreamer: A tool for synthetic data generation and reproducible llm workflows[J]. arXiv preprint arXiv:2402.10379, 2024. [[pdf]](https://arxiv.org/abs/2402.10379) [[code]](https://github.com/datadreamer-dev/DataDreamer)

[5] Yang D, Monaikul N, Ding A, et al. Enhancing Table Representations with LLM-powered Synthetic Data Generation[J]. arXiv preprint arXiv:2411.03356, 2024. [[pdf]](https://arxiv.org/abs/2411.03356) 

[6] Pentyala S, Pereira M, De Cock M. Caps: Collaborative and private synthetic data generation from distributed sources[J]. arXiv preprint arXiv:2402.08614, 2024. [[pdf]](https://arxiv.org/abs/2402.08614) [[code]](https://github.com/sikhapentyala/MPC_SDG/tree/icml)

[7] Zhou Z, Yu K Y, Tian S Y, et al. Lawgpt: Knowledge-guided data generation and its application to legal llm[J]. arXiv preprint arXiv:2502.06572, 2025. [[pdf]](https://arxiv.org/abs/2502.06572) [[code]](https://github.com/LAMDASZ-ML/Knowledge-Guide-Data-Generation)

[8] Shimabucoro L, Ruder S, Kreutzer J, et al. Llm see, llm do: Guiding data generation to target non-differentiable objectives, 2024[J]. URL https://arxiv. org/abs/2407.01490.  [[pdf]](https://arxiv.org/abs/2407.01490) [[code]](https://github.com/Cohere-Labs-Community/llm-profiling-toolkit)

[9] Mishra A, Nayak G, Bhattacharya S, et al. Llm-guided counterfactual data generation for fairer ai[C]//Companion Proceedings of the ACM Web Conference 2024. 2024: 1538-1545. [[pdf]](https://dl.acm.org/doi/10.1145/3589335.3651929) 

[10] Banday B, Thopalli K, Islam T Z, et al. On the role of prompt construction in enhancing efficacy and efficiency of llm-based tabular data generation[C]//ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025: 1-5. [[pdf]](https://ieeexplore.ieee.org/abstract/document/10888077) 

[11] Mahabadi R K, Satheesh S, Prabhumoye S, et al. Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset[J]. arXiv preprint arXiv:2508.15096, 2025. [[pdf]](https://arxiv.org/abs/2508.15096) [[code]](https://github.com/NVIDIA-NeMo/Curator)

[12] Pan Z, Wang S, Li J. Understanding llm behaviors via compression: Data generation, knowledge acquisition and scaling laws[J]. arXiv preprint arXiv:2504.09597, 2025. [[pdf]](https://arxiv.org/abs/2504.09597) 

[13] Zhu Y, Zhong H, Lin Q, et al. What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning[J]. arXiv preprint arXiv:2506.19262, 2025. [[pdf]](https://arxiv.org/abs/2506.19262) 
